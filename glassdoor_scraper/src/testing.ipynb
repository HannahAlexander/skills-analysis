{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from scraping_functions.search_result import get_URL_content\n",
    "from scraping_functions.search_result import build_full_urls\n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "# os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.sg/Job/united-kingdom-data-scientist-jobs-SRCH_IL.0,14_IN2_KO15,29.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL content from the Glassdoor search result\n",
    "search_soup, _ = get_URL_content(url)\n",
    "\n",
    "# Return full urls from search soup\n",
    "all_urls_scraped = build_full_urls(search_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_urls_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "urls1 = set(random.sample(list(all_urls_scraped), 20))\n",
    "\n",
    "random.seed(2)\n",
    "urls2 = set(random.sample(list(all_urls_scraped), 20))\n",
    "\n",
    "random.seed(3)\n",
    "urls3 = set(random.sample(list(all_urls_scraped), 20))\n",
    "\n",
    "urls23 = urls2.copy()\n",
    "urls23.update(urls3) # len=28\n",
    "\n",
    "urls12 = urls1.copy()\n",
    "urls12.update(urls2) #len = 28\n",
    "\n",
    "urls13 = urls1.copy()\n",
    "urls13.update(urls3) #len = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url_csv(scraped_url_csv_name: str) -> set:\n",
    "    \"\"\"\n",
    "    Takes in string of csv where previously scraped urls are stored.\n",
    "    Returns set of urls.\n",
    "    If csv does not exist, create an empty set to write to csv later.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(scraped_url_csv_name, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "\n",
    "            # transform list of lists to set\n",
    "            saved_urls = {i for lst in list(reader) for i in lst}\n",
    "\n",
    "        print(\"Successfully read \", len(saved_urls), \" urls from\", scraped_url_csv_name)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR]\", e)\n",
    "        saved_urls = set()\n",
    "        print(\"Created empty set of urls\")\n",
    "    return saved_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_urls(all_urls_scraped: set, saved_urls: set) -> tuple[set, int]:\n",
    "    \"\"\"\n",
    "    Takes all_urls_scraped from build_full_urls and saved_urls from read_url_csv\n",
    "    and returns unique_urls which have not been saved/scraped before.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # compare sets\n",
    "        unique_urls = all_urls_scraped.difference(saved_urls)\n",
    "        print(\"urls successfully compared\")\n",
    "\n",
    "        # find length of resultings lists\n",
    "        no_unique_urls = len(unique_urls)\n",
    "        repeated_urls = len(all_urls_scraped) - no_unique_urls\n",
    "\n",
    "        print(\"[INFO]\", no_unique_urls, \"unique urls found\")\n",
    "        print(\"[INFO]\", repeated_urls, \"repeated urls found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Error has occured in get_unique_urls\")\n",
    "        print(e)\n",
    "\n",
    "    return unique_urls, no_unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_file_writer(unique_urls: set, scraped_url_csv_name: str):\n",
    "    \"\"\"\n",
    "    Take in set of unique_urls and name of output csv and apends set to csv\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(scraped_url_csv_name, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(list(unique_urls))\n",
    "            f.close()\n",
    "        print(\"[INFO]\", len(unique_urls), \"urls saved to \", scraped_url_csv_name)\n",
    "    # Print error if necessary\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Problem occurred in file_writer: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_urls = read_url_csv(test_csv)\n",
    "saved_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls, no_unique_urls = get_unique_urls(urls1, saved_urls)\n",
    "# unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_file_writer(unique_urls, test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_urls2 = read_url_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls, no_unique_urls = get_unique_urls(urls12, saved_urls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_file_writer(unique_urls, test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_urls3 = read_url_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls, no_unique_urls = get_unique_urls(urls3, saved_urls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_file_writer(unique_urls, test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_urls4 = read_url_csv(test_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mba')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d922b89ffc6c4a65d1b82e472aa0d2049cad8961dcfee598f26185a265df87f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
